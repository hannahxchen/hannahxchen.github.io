---
layout: post
title: 'Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control'
date: 2025-04-24
tags: ["LLMs", "censorship"]
categories: ["paper"]
description: We examine safety-tuned LLMs and discover representation vectors for measuring and controlling censorship imposed through refusal and thought suppression in model outputs.
---

<div class="image-container">
  <img src="/assets/img/censorship-steering/cover-photo.png" style="max-height: 320px;"/>
</div>

## Censorship on LLM Outputs
Large language models (LLMs) have enabled new ways to access information through interactions with chatbots and AI assistants. To address potential harms and misuse, ‚Äúsafety‚Äù training methods have become essential in LLM development. Models are trained to refuse harmful requests while providing responses that aligned with human preferences and usage policies. Similar to censorship, the goal is to restrict model behaviors to a ‚Äúsafe‚Äù or ‚Äúpreferred‚Äù subset of outputs [^jailbroken]. However, Model providers have control over what they deem as ‚Äúharmful‚Äù and what should be censored. Corporate interests and government policies may incentivize them to impose certain values or beliefs on outputs generated by their models, posing threats to freedom of expression and individual autonomy.

#### Censorship in DeepSeek-R1 Distilled Models
While the most common censoring behavior of an instruction LLM is *refusal*, we investigate recent reasoning LLMs distilled from DeepSeek-R1 model [^deepseekR1] and find a distinct type of censorship through "thought suppression". Specifically, the model tends to bypass the reasoning process by outputting "&lt;think&gt;\n\n&lt;/think&gt;", instead of the regular chain-of-thoughts, such as "&lt;think&gt;\n*Okay, so I need to figure out*...&lt;/think&gt;". We find this pattern particularly prominent in model responses to prompts related to sensitive topics in China [^prompt], compared to regular harmless prompts and even harmful jailbreak prompts.

<div class="image-container">
  <img src="/assets/img/censorship-steering/censored-percentage.png" style="max-height: 230px;"/>
  <figcaption>Percentage of censored outputs by prompt type. (refuse: direct refusal string; ¬¨think: thinking bypassed)</figcaption>
</div>


## Finding Censorship Steering Vectors
Building on recent work on *representation engineering* [^repEng], we analyze the internal representations of instruction and reasoning LLMs to understand the censorship mechanism that suppresses certain outputs from being produced by the model. In addition, we explore methods for detecting and controlling the level of censorship in model outputs.

We adapt our recently proposed method [^steering] to find a steering vector for LLM censorship. We assume this vector lies along a single dimension within a model's representation space, as illustrated in the figure below:

<div class="image-container">
  <img src="/assets/img/censorship-steering/steering-vector.png" style="height: 300px;"/>
</div>

An input prompt can be mapped to a point in the representation space, corresponding to the model‚Äôs internal activation. We extract a candidate vector from each model layer by computing the activation difference between inputs with censored outputs (refusal) and non-censored outputs (compliance), and then select a single vector that best corresponds to the model‚Äôs censoring behavior. Since a model may show varied levels of censorship for different inputs, we weight the activations based on their censorship probability, which we inferr from the outputs. 

To apply the steering vector, we use a coefficient Œª to control the direction and degree of steering. Setting Œª < 0 reduces censorship in model outputs, whereas Œª > 0 imposes more censorship.


### Eliminating "Thought Suppression"
We exploit the censorship-revealing thinking pattern of DeepSeek-R1 to derive steering vectors for controlling censorship on the model‚Äôs thought process. With the ‚Äúthought suppression‚Äù vector we found, we can detect potential censorship in model outputs by measuring the input‚Äôs projection on the vector. We can also apply this vector with varying coefficients to manipulate the probability of the model bypassing its thought process.

<div class="image-container">
    <div style="max-height: 250px; display: flex; justify-content: center;">
      <img src="/assets/img/censorship-steering/projection-deepseek-qwen-7b.png" style="max-width: 50%;">
      <img src="/assets/img/censorship-steering/steering-thought-suppression.png" style="max-width: 50%;">
    </div>
     <figcaption>‚ÄúThought suppression‚Äù vector of DeepSeek-R1-Distill-Qwen-7B. Left: Projections and thought suppression scores measured for each prompt, where a score of 1 indicates full suppression and -1 means zero suppression. Right: The average next token probability of ‚Äú\n\n‚Äù (stop thinking) and ‚Äú\n‚Äù (start thinking) with different steering coefficient applied to the model.</figcaption>
</div>

Here, we show several examples generated by steering DeepSeek-R1-Distill-Qwen-7B. The model provides a description of the Tiananmen Square incident when we apply the steering vector with a negative coefficient. In some cases, we evade the ‚Äúthought suppression‚Äù but the model‚Äôs response remains ‚Äúcensored‚Äù. For example, when asking for code words for ‚ÄúWinnie the Pooh‚Äù, the thinking process suggests possible ‚Äúharmful‚Äù intent, which leads to refusal in the final answer. However, we find that we can circumvent this by using a larger negative coefficient. Similarly, when discussing Taiwan‚Äôs independence, the model initially maintains the One-China principle. We can steer the model to acknowledge the complexity of the issue and ultimately recognize Taiwan as an independent nation. These results demonstrate our method‚Äôs potential in countering censorship in reasoning LLMs.

{% include censorship-steering/deepseek-r1_examples.liquid %}


### Steering *Refusal‚ÄîCompliance* in Instruction Models
We find *refusal‚Äîcompliance* vectors for instruction LLMs and evaluate their steering performance over several red-teaming benchmarks. We visualize the average probability of refusal and harmful response with varying coefficients below.

<div class="image-container">
  <img src="/assets/img/censorship-steering/steering-refusal-compliance.png" style="max-height: 250px;"/>
  <figcaption>The average refusal and harmful response probability measured by WildGuard&nbsp;<span markdown="1">[^wildguard]</span>.</figcaption>
</div>

Steering with a more negative coefficient decreases the model's refusal rate but increases the harmful response probability. The refusal probability gradually increases with a higher Œª value. This shows that our method can effectively control the degree of refusal-based censorship in model outputs.

Below shows a few examples generated by steering Llama-2-7B-Chat:
{: style="margin-bottom: .5rem;"}

{% include censorship-steering/refusal_examples.liquid %}


**TL;DR:** Our work examines safety-tuned LLMs and discover representation vectors for measuring and controlling censorship imposed through refusal and thought suppression in model outputs.

**Paper**: [Hannah Cyberey](https://hannahxchen.github.io/) and [David Evans](http://www.cs.virginia.edu/~evans/). [*Steering the CensorShip: Uncovering Representation Vectors for LLM ‚ÄúThought‚Äù Control*](https://arxiv.org/abs/2504.17130). In arXiv Preprint, April 2025.<br>
**Demos**:
{: style="margin-bottom: 0;"}
- üê≥ [Steering Thought Suppression](https://mightbeevil.com/censorship) with DeepSeek-R1-Distill-Qwen-7B
- ü¶ô [Steering Refusal‚ÄîCompliance](https://hannahcyberey-refusal-censorship-steering.hf.space/) with Llama-3.1-8B-Instruct
{: style="margin-bottom: .5em;"}
**Code**: [https://github.com/hannahxchen/llm-censorship-steering](https://github.com/hannahxchen/llm-censorship-steering)
{: style="line-height: 1.65em;"}

<hr>

[^jailbroken]: Wei et al., 2023. [Jailbroken: How Does LLM Safety Training Fail?](http://arxiv.org/abs/2307.02483)
[^repEng]: Zou et al., 2023. [Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/abs/2310.01405)
[^refusal]: Arditi et al., 2024. [Refusal in Language Models Is Mediated by a Single Direction](https://arxiv.org/abs/2406.11717)
[^steering]: Cyberey et al., 2025. [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/abs/2502.19721)
[^wildguard]: Han et al., 2024. [WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs](https://arxiv.org/abs/2406.18495)
[^deepseekR1]: DeepSeek-AI, 2025. [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)
[^prompt]: The sensitive prompts are based on [CCP-Sensitive](https://huggingface.co/datasets/promptfoo/CCP-sensitive-prompts) and [deccp](https://huggingface.co/datasets/augmxnt/deccp) datasets.

<style>
  html[data-theme="dark"] {
    figcaption {
      color: white;
    }
  }

  div.image-container {
    padding-top: 1em;
    padding-bottom: 1em;
    display: flex;
    justify-content: center;
    align-content: center;
    flex-wrap: wrap;
    img {
        max-width: 100%; 
        max-height: 95%; 
        display: block; 
        margin: auto;
    }
  }

  figcaption {
      font-style: italic;
      font-size: 1em;
      color: dimgray;
      display: flex;
      justify-content: center;
      padding-top: .3em;
      padding-bottom: .8em;
  }

  div.examples {
      padding-bottom: 2rem;

      .nav-pills {
        margin-bottom: .6rem;
        padding-top: .5rem;
      }

      .tab-content {
          padding: 10px 20px 5px 20px;
          border-radius: .6rem; 
          background-color: aliceblue;

          p {
              font-size: 1.06rem;
          }
      }

      p.example-caption {
          text-align: center;
          color: dimgray;
          font-size: 1rem;
          margin-top: .5em;
          margin-bottom: 0em;
      }
  }


  .nav-pills .nav-item {
      padding: .25em 1em;
      font-weight: normal;
      
      .nav-link {
        font-size: 1.25rem;
        color: #000000;
        background-color: aliceblue;
        border-radius: .6rem;
        padding: 0.3em 0.8em;
        height: 100%;
        display: flex;
        align-items: center;
        justify-content: center;
      }

      .nav-link.active {
          color: white;
          font-weight: 420;
          background-color: #0094ffcc;
      }

      .nav-link:hover {
          color: white;
          font-weight: 420;
          background-color: #0094ffcc;
      }
  }

  div.tab-pane {
    p.instruction {
        margin-bottom: 0em;
        color: #000000;
    }

    b {
      font-weight: bold;
    }

    hr {
      border-top: 1px solid rgba(0, 0, 0, .1);
      margin-top: .8rem;
      margin-bottom: .8rem;
    }
  }

  div.steering-output{
      height: 350px;
      p {
          color: #000000;
          margin-bottom: 0.15em;
      }
      p.output-title {
          text-align: center;
          font-size: 1.1rem;
          margin-top: 0.8em;
          margin-bottom: 0em;
          color: darkslateblue;
      }
  }

</style>
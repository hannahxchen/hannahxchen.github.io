---
---
@inproceedings{cyberey2025steering,
    abbr = {COLM},
    title = {Steering the CensorShip: Uncovering Representation Vectors for LLM ``Thought'' Control}, 
    author = {Hannah Cyberey and David Evans},
    booktitle = {Conferenc on Languagee Modeling (COLM)},
    year = {2025},
    month = oct,
    arxiv = {2504.17130},
    url = {https://arxiv.org/abs/2504.17130}, 
    code = "https://github.com/hannahxchen/llm-censorship-steering",
    abstract = "Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this ``censorship'' works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through ``thought suppression''. We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector"
}

@inproceedings{cyberey-etal-2025-prevalent,
    abbr = {Insights Workshop},
    title = {Do Prevalent Bias Metrics Capture Allocational Harms from {LLM}s?},
    author = {Hannah Cyberey and Yangfeng Ji and David Evans},
    booktitle = "The Sixth Workshop on Insights from Negative Results in NLP",
    month = may,
    year = "2025",
    publisher = "Association for Computational Linguistics",
    acl = "https://aclanthology.org/2025.insights-1.5/",
    abstract = "Allocational harms occur when resources or opportunities are unfairly withheld from specific groups. Many proposed bias measures ignore the discrepancy between predictions, which are what the proposed methods consider, and decisions that are made as a result of those predictions. Our work examines the reliability of current bias metrics in assessing allocational harms arising from predictions of large language models (LLMs). We evaluate their predictive validity and utility for model selection across ten LLMs and two allocation tasks. Our results reveal that commonly-used bias metrics based on average performance gap and distribution distance fail to reliably capture group disparities in allocation outcomes. Our work highlights the need to account for how model predictions are used in decisions, in particular in contexts where they are influenced by how limited resources are allocated."
}

@article{cyberey2025sensing,
    abbr = {Preprint},
    title = {Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs}, 
    author = {Hannah Cyberey and Yangfeng Ji and David Evans},
    month = feb,
    year = {2025},
    journal = "ArXiv Preprint",
    arxiv = {2502.19721},
    url = {https://arxiv.org/abs/2502.19721}, 
    code = "https://github.com/hannahxchen/gender-bias-steering",
    abstract = "Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of ``gender'' is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model's representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs."
}

@article{chen2024mismeasure,
    abbr = {Preprint},
    title = {The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language Models}, 
    author = {Hannah Cyberey and Yangfeng Ji and David Evans},
    month = aug,
    year = {2024},
    journal = "ArXiv Preprint",
    arxiv = {2408.01285},
    url = {https://arxiv.org/abs/2408.01285}, 
    code = "https://github.com/hannahxchen/llm-allocational-harm-eval",
    abstract = "Large language models (LLMs) are now being considered and even deployed for applications that support high-stakes decision-making, such as recruitment and clinical decisions. While several methods have been proposed for measuring bias, there remains a gap between predictions, which are what the proposed methods consider, and how they are used to make decisions. In this work, we introduce Rank-Allocational-Based Bias Index (RABBI), a model-agnostic bias measure that assesses potential allocational harms arising from biases in LLM predictions. We compare RABBI and current bias metrics on two allocation decision tasks. We evaluate their predictive validity across ten LLMs and utility for model selection. Our results reveal that commonly-used bias metrics based on average performance gap and distribution distance fail to reliably capture group disparities in allocation outcomes, whereas RABBI exhibits a strong correlation with allocation disparities. Our work highlights the need to account for how models are used in contexts with limited resource constraints."
}

@inproceedings{chen-etal-2024-addressing,
    abbr = {NAACL Findings},
    title = "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
    author = "Cyberey, Hannah  and Ji, Yangfeng  and Evans, David",
    booktitle = "Findings of NAACL 2024",
    month = jun,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    acl = "https://aclanthology.org/2024.findings-naacl.38",
    arxiv = "2404.00463",
    code = "https://github.com/hannahxchen/composed-debiasing",
    abstract = "Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics."
}

@inproceedings{chen-etal-2022-balanced,
    abbr = {EMNLP},
    title = "Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models",
    author = "Cyberey, Hannah  and Ji, Yangfeng  and Evans, David",
    booktitle = "EMNLP 2022",
    month = dec,
    year = "2022",
    publisher = "Association for Computational Linguistics",
    acl = "https://aclanthology.org/2022.emnlp-main.40",
    arxiv = "2210.11498",
    code = "https://github.com/hannahxchen/balanced-adversarial-training",
    abstract = "Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input's true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves the classifier's prediction but changes the true label of an input. Adversarial training and certified robust training have shown some effectiveness in improving the robustness of machine learnt models to fickle adversarial examples. We show that standard adversarial training methods focused on reducing vulnerability to fickle adversarial examples may make a model more vulnerable to obstinate adversarial examples, with experiments for both natural language inference and paraphrase identification tasks. To counter this phenomenon, we introduce Balanced Adversarial Training, which incorporates contrastive learning to increase robustness against both fickle and obstinate adversarial examples."
}

@inproceedings{chen-etal-2020-finding,
    abbr = {EMNLP Findings},
    title = "Finding {F}riends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory",
    author = "Cyberey, Hannah  and Ji, Yangfeng  and Evans, David",
    booktitle = "Findings of EMNLP 2020",
    month = nov,
    year = "2020",
    publisher = "Association for Computational Linguistics",
    acl = "https://aclanthology.org/2020.findings-emnlp.426",
    arxiv = "2011.01856",
    code = "https://github.com/hannahxchen/automatic-paraphrase-dataset-augmentation",
    abstract = "Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an augmented dataset by directly inferring labels from the original sentence pairs using a transitivity property. We use structural balance theory to identify likely mislabelings in the graph, and flip their labels. We evaluate our methods on paraphrase models trained using these datasets starting from a pretrained BERT model, and find that the automatically-enhanced training sets result in more accurate models.",
}

@inproceedings{chen-etal-2020-pointwise,
    abbr = {ACL SRW},
    title = "Pointwise Paraphrase Appraisal is Potentially Problematic",
    author = "Cyberey, Hannah  and Ji, Yangfeng  and Evans, David",
    booktitle = "ACL 2020 Student Research Workshop",
    month = jul,
    year = "2020",
    publisher = "Association for Computational Linguistics",
    acl = "https://aclanthology.org/2020.acl-srw.20",
    arxiv = "2005.11996",
    abstract = "The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models. As a first step towards that goal, we show that although the standard way of fine-tuning BERT for paraphrase identification by pairing two sentences as one sequence results in a model with state-of-the-art performance, that model may perform poorly on simple tasks like identifying pairs with two identical sentences. Moreover, we show that these models may even predict a pair of randomly-selected sentences with higher paraphrase score than a pair of identical ones.",
}
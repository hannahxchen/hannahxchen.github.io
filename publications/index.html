<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      publications | Hannah Cyberey
    
  
</title>
<meta name="author" content="Hannah Cyberey">
<meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar.">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/publications/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script>

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Hannah</span>
            
            
            Cyberey
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/publications/">publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/misc/">misc
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p>
  </header>

  <article>
    <!-- _pages/publications.md -->
<div class="publications">

<h2 class="bibliography">2025</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#ff8b00"><a href="">COLM</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="cyberey2025steering" class="col-sm-8">
    <!-- Title -->
    <div class="title">Steering the CensorShip: Uncovering Representation Vectors for LLM “Thought” Control</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Conferenc on Languagee Modeling (COLM)</em> ,  Oct 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2504.17130" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/hannahxchen/llm-censorship-steering" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this “censorship” works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal–compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through “thought suppression”. We show a similar approach can be used to find a vector that suppresses the model’s reasoning process, allowing us to remove censorship by applying the negative multiples of this vector</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#f5de00"><a href="">Insights Workshop</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="cyberey-etal-2025-prevalent" class="col-sm-8">
    <!-- Title -->
    <div class="title">Do Prevalent Bias Metrics Capture Allocational Harms from LLMs?</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Sixth Workshop on Insights from Negative Results in NLP</em> ,  May 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
        <a href="https://aclanthology.org/2025.insights-1.5/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a>
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Allocational harms occur when resources or opportunities are unfairly withheld from specific groups. Many proposed bias measures ignore the discrepancy between predictions, which are what the proposed methods consider, and decisions that are made as a result of those predictions. Our work examines the reliability of current bias metrics in assessing allocational harms arising from predictions of large language models (LLMs). We evaluate their predictive validity and utility for model selection across ten LLMs and two allocation tasks. Our results reveal that commonly-used bias metrics based on average performance gap and distribution distance fail to reliably capture group disparities in allocation outcomes. Our work highlights the need to account for how model predictions are used in decisions, in particular in contexts where they are influenced by how limited resources are allocated.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#2698ba"><a href="">Preprint</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="cyberey2025sensing" class="col-sm-8">
    <!-- Title -->
    <div class="title">Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In ArXiv Preprint</em>,  Feb 2025
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2502.19721" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/hannahxchen/gender-bias-steering" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of “gender” is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model’s representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#2698ba"><a href="">Preprint</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chen2024mismeasure" class="col-sm-8">
    <!-- Title -->
    <div class="title">The Mismeasure of Man and Models: Evaluating Allocational Harms in Large Language Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In ArXiv Preprint</em>,  Aug 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2408.01285" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
      
      
      
      
      
      
      
        <a href="https://github.com/hannahxchen/llm-allocational-harm-eval" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large language models (LLMs) are now being considered and even deployed for applications that support high-stakes decision-making, such as recruitment and clinical decisions. While several methods have been proposed for measuring bias, there remains a gap between predictions, which are what the proposed methods consider, and how they are used to make decisions. In this work, we introduce Rank-Allocational-Based Bias Index (RABBI), a model-agnostic bias measure that assesses potential allocational harms arising from biases in LLM predictions. We compare RABBI and current bias metrics on two allocation decision tasks. We evaluate their predictive validity across ten LLMs and utility for model selection. Our results reveal that commonly-used bias metrics based on average performance gap and distribution distance fail to reliably capture group disparities in allocation outcomes, whereas RABBI exhibits a strong correlation with allocation disparities. Our work highlights the need to account for how models are used in contexts with limited resource constraints.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#dd2382"><a href="">NAACL Findings</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chen-etal-2024-addressing" class="col-sm-8">
    <!-- Title -->
    <div class="title">Addressing Both Statistical and Causal Gender Fairness in NLP Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Findings of NAACL 2024</em> ,  Jun 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2404.00463" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a href="https://aclanthology.org/2024.findings-naacl.38" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a>
      
      
      
      
      
      
      
      
        <a href="https://github.com/hannahxchen/composed-debiasing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#5b9113"><a href="">EMNLP</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chen-etal-2022-balanced" class="col-sm-8">
    <!-- Title -->
    <div class="title">Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In EMNLP 2022</em> ,  Dec 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2210.11498" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a href="https://aclanthology.org/2022.emnlp-main.40" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a>
      
      
      
      
      
      
      
      
        <a href="https://github.com/hannahxchen/balanced-adversarial-training" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input’s true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves the classifier’s prediction but changes the true label of an input. Adversarial training and certified robust training have shown some effectiveness in improving the robustness of machine learnt models to fickle adversarial examples. We show that standard adversarial training methods focused on reducing vulnerability to fickle adversarial examples may make a model more vulnerable to obstinate adversarial examples, with experiments for both natural language inference and paraphrase identification tasks. To counter this phenomenon, we introduce Balanced Adversarial Training, which incorporates contrastive learning to increase robustness against both fickle and obstinate adversarial examples.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#5b9113"><a href="">EMNLP Findings</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chen-etal-2020-finding" class="col-sm-8">
    <!-- Title -->
    <div class="title">Finding Friends and Flipping Frenemies: Automatic Paraphrase Dataset Augmentation Using Graph Theory</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Findings of EMNLP 2020</em> ,  Nov 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2011.01856" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a href="https://aclanthology.org/2020.findings-emnlp.426" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a>
      
      
      
      
      
      
      
      
        <a href="https://github.com/hannahxchen/automatic-paraphrase-dataset-augmentation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Most NLP datasets are manually labeled, so suffer from inconsistent labeling or limited size. We propose methods for automatically improving datasets by viewing them as graphs with expected semantic properties. We construct a paraphrase graph from the provided sentence pair labels, and create an augmented dataset by directly inferring labels from the original sentence pairs using a transitivity property. We use structural balance theory to identify likely mislabelings in the graph, and flip their labels. We evaluate our methods on paraphrase models trained using these datasets starting from a pretrained BERT model, and find that the automatically-enhanced training sets result in more accurate models.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
        
          
          
          <abbr class="badge" style="background-color:#f5de00"><a href="">ACL SRW</a></abbr>
        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chen-etal-2020-pointwise" class="col-sm-8">
    <!-- Title -->
    <div class="title">Pointwise Paraphrase Appraisal is Potentially Problematic</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Hannah
            Cyberey</em>, Yangfeng
            Ji
          , and David
            Evans
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In ACL 2020 Student Research Workshop</em> ,  Jul 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
        <a href="http://arxiv.org/abs/2005.11996" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a href="https://aclanthology.org/2020.acl-srw.20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">ACL</a>
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The prevailing approach for training and evaluating paraphrase identification models is constructed as a binary classification problem: the model is given a pair of sentences, and is judged by how accurately it classifies pairs as either paraphrases or non-paraphrases. This pointwise-based evaluation method does not match well the objective of most real world applications, so the goal of our work is to understand how models which perform well under pointwise evaluation may fail in practice and find better methods for evaluating paraphrase identification models. As a first step towards that goal, we show that although the standard way of fine-tuning BERT for paraphrase identification by pairing two sentences as one sequence results in a model with state-of-the-art performance, that model may perform poorly on simple tasks like identifying pairs with two identical sentences. Moreover, we show that these models may even predict a pair of randomly-selected sentences with higher paraphrase score than a pair of identical ones.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2025
      Hannah
      
      Cyberey. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script>
<script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
      },
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>


    
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-128597383-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      window.dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-128597383-1');
  </script>



    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

  </body>
</html>
